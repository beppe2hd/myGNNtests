{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import OGB_MAG\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from torch_geometric.nn import SAGEConv, GATConv, Linear, HGTConv, HeteroConv, GCNConv, to_hetero\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "dataset = OGB_MAG(root='./data', preprocess='metapath2vec', transform=T.ToUndirected())\n",
    "data = dataset[0]\n",
    "num_of_class = dataset.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv((-1, -1), hidden_channels)\n",
    "        self.conv2 = SAGEConv((-1, -1), out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x['paper']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = HeteroConv({\n",
    "            ('paper', 'cites', 'paper'): GCNConv(-1, hidden_channels),\n",
    "            ('author', 'writes', 'paper'): SAGEConv((-1, -1), hidden_channels),\n",
    "            ('paper', 'rev_writes', 'author'): SAGEConv((-1, -1), hidden_channels),\n",
    "        }, aggr='sum')\n",
    "\n",
    "        self.conv2 = HeteroConv({\n",
    "            ('paper', 'cites', 'paper'): SAGEConv((-1, -1), hidden_channels),\n",
    "            ('author', 'writes', 'paper'): SAGEConv((-1, -1), hidden_channels),\n",
    "            ('paper', 'rev_writes', 'author'): SAGEConv((-1, -1), hidden_channels),\n",
    "        }, aggr='sum')\n",
    "\n",
    "        self.conv3 = HeteroConv({\n",
    "            ('paper', 'cites', 'paper'): GCNConv(-1, hidden_channels),\n",
    "            ('author', 'writes', 'paper'): SAGEConv((-1, -1), hidden_channels),\n",
    "            ('paper', 'rev_writes', 'author'): SAGEConv((-1, -1), hidden_channels),\n",
    "        }, aggr='sum')\n",
    "\n",
    "        self.lin = Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        x_dict = self.conv1(x_dict, edge_index_dict)\n",
    "        x_dict = {key: x.relu() for key, x in x_dict.items()}\n",
    "        x_dict = self.conv2(x_dict, edge_index_dict)\n",
    "        x_dict = {key: x.relu() for key, x in x_dict.items()}\n",
    "        x_dict = self.conv3(x_dict, edge_index_dict)\n",
    "        x_dict = {key: x.relu() for key, x in x_dict.items()}\n",
    "\n",
    "        return self.lin(x_dict['paper'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HGT(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, hidden_channels2, hidden_channels3, out_channels, num_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lin_paper = Linear(128, hidden_channels)\n",
    "        self.lin_author = Linear(128, hidden_channels)\n",
    "        self.lin_institution = Linear(128, hidden_channels)\n",
    "        self.lin_field_of_study = Linear(128, hidden_channels)\n",
    "\n",
    "        self.conv1 = HGTConv(hidden_channels, hidden_channels2, data.metadata(),\n",
    "                       num_heads, group='sum')\n",
    "        self.conv2 = HGTConv(hidden_channels2, hidden_channels3, data.metadata(),\n",
    "                       num_heads, group='sum')\n",
    "\n",
    "        self.lin = Linear(hidden_channels3, out_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        for node_type, x in x_dict.items():\n",
    "            if node_type == 'paper':\n",
    "                x_dict[node_type] = self.lin_paper(x).relu_()\n",
    "            if node_type == 'author':\n",
    "                x_dict[node_type] = self.lin_author(x).relu_()\n",
    "            if node_type == 'institution':\n",
    "                x_dict[node_type] = self.lin_institution(x).relu_()\n",
    "            if node_type == 'field_of_study':\n",
    "                x_dict[node_type] = self.lin_field_of_study(x).relu_()\n",
    "\n",
    "        x_dict = self.conv1(x_dict, edge_index_dict)\n",
    "        x_dict = self.conv2(x_dict, edge_index_dict)\n",
    "\n",
    "        return self.lin(x_dict['paper'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_examples = total_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        batch_size = batch['paper'].batch_size\n",
    "        out = model(batch.x_dict, batch.edge_index_dict)\n",
    "        loss = criterion(out[:batch_size], batch['paper'].y[:batch_size])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_examples += batch_size\n",
    "        total_loss += float(loss) * batch_size\n",
    "\n",
    "    return total_loss / total_examples\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "\n",
    "    total_examples = total_loss = 0\n",
    "    for batch in tqdm(test_loader):\n",
    "        batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        batch_size = batch['paper'].batch_size\n",
    "        out = model(batch.x_dict, batch.edge_index_dict)\n",
    "\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct = pred[:batch_size] == batch['paper'].y[:batch_size]\n",
    "        accs = int(correct.sum()) / int(batch_size)\n",
    "        return accs\n",
    "\n",
    "\n",
    "    return total_loss / total_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = NeighborLoader(\n",
    "    data,\n",
    "    # Sample 15 neighbors for each node and each edge type for 2 iterations:\n",
    "    num_neighbors=[15] * 2,\n",
    "    # Use a batch size of 128 for sampling training nodes of type \"paper\":\n",
    "    batch_size=128,\n",
    "    input_nodes=('paper', data['paper'].train_mask),\n",
    ")\n",
    "\n",
    "test_loader = NeighborLoader(\n",
    "    data,\n",
    "    # Sample 15 neighbors for each node and each edge type for 2 iterations:\n",
    "    num_neighbors=[15] * 2,\n",
    "    # Use a batch size of 128 for sampling training nodes of type \"paper\":\n",
    "    batch_size=128,\n",
    "    input_nodes=('paper', data['paper'].test_mask),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n",
    "numofep = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_GNN = []\n",
    "acc_GNN = []\n",
    "model= GNN(hidden_channels=64, out_channels=num_of_class)\n",
    "model = to_hetero(model, data.metadata(), aggr='sum')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)  # Define optimizer.\n",
    "model.to(device)\n",
    "for i in range(1,numofep):\n",
    "    print(f\"Epoch {i}\")\n",
    "    loss = train()\n",
    "    print(f\"Current losso is: {loss}\")\n",
    "    loss_GNN.append(loss)\n",
    "    acc_GNN.append(test())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_HeteroGNN = []\n",
    "acc_HeteroGNN = []\n",
    "model= HeteroGNN(hidden_channels=64, out_channels=num_of_class)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)  # Define optimizer.\n",
    "model.to(device)\n",
    "for i in range(1,numofep):\n",
    "    print(f\"Epoch {i}\")\n",
    "    loss = train()\n",
    "    print(f\"Current losso is: {loss}\")\n",
    "    loss_HeteroGNN.append(loss)\n",
    "    acc_HeteroGNN.append(test())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_HGT = []\n",
    "acc_HGT= []\n",
    "model = HGT(hidden_channels=64, hidden_channels2=32, hidden_channels3=16, out_channels=num_of_class, num_heads=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)  # Define optimizer.\n",
    "model.to(device)\n",
    "for i in range(1,numofep):\n",
    "    print(f\"Epoch {i}\")\n",
    "    loss = train()\n",
    "    print(f\"Current losso is: {loss}\")\n",
    "    loss_HGT.append(loss)\n",
    "    acc_HGT.append(train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plot lines\n",
    "plt.plot(loss_HeteroGNN, label=\"loss_HeteroGNN\")\n",
    "plt.plot(loss_HGT, label=\"loss_HGT\")\n",
    "plt.legend()\n",
    "plt.savefig(\"loss.png\")\n",
    "\n",
    "plt.close()\n",
    "\n",
    "plt.plot(acc_HeteroGNN, label=\"acc_HeteroGNN\")\n",
    "plt.plot(acc_HGT, label=\"acc_HGT\")\n",
    "plt.legend()\n",
    "plt.savefig(\"acc.png\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
