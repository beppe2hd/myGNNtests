{"cells":[{"cell_type":"code","execution_count":48,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31064,"status":"ok","timestamp":1650958085119,"user":{"displayName":"Matthias Fey","userId":"07888747774515796046"},"user_tz":-120},"id":"F1op-CbyLuN4","outputId":"77aaf048-dc7e-491d-a0ed-7a13f3692cc0"},"outputs":[{"name":"stdout","output_type":"stream","text":["2.0.1\n"]}],"source":["# Install required packages.\n","import os\n","import torch\n","os.environ['TORCH'] = torch.__version__\n","print(torch.__version__)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"dszt2RUHE7lW"},"source":["# Node Classification with Graph Neural Networks\n","\n","This tutorial will teach you how to apply **Graph Neural Networks (GNNs) to the task of node classification**.\n","Here, we are given the ground-truth labels of only a small subset of nodes, and want to infer the labels for all the remaining nodes (*transductive learning*).\n","\n","To demonstrate, we make use of the `Cora` dataset, which is a **citation network** where nodes represent documents.\n","Each node is described by a 1433-dimensional bag-of-words feature vector.\n","Two documents are connected if there exists a citation link between them.\n","The task is to infer the category of each document (7 in total).\n","\n","This dataset was first introduced by [Yang et al. (2016)](https://arxiv.org/abs/1603.08861) as one of the datasets of the `Planetoid` benchmark suite.\n","We again can make use [PyTorch Geometric](https://github.com/rusty1s/pytorch_geometric) for an easy access to this dataset via [`torch_geometric.datasets.Planetoid`](https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html#torch_geometric.datasets.Planetoid):"]},{"cell_type":"code","execution_count":49,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4790,"status":"ok","timestamp":1650958097348,"user":{"displayName":"Matthias Fey","userId":"07888747774515796046"},"user_tz":-120},"id":"imGrKO5YH11-","outputId":"2a64e675-158c-4519-ab28-1923c4ab830a"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Dataset: Cora():\n","======================\n","Number of graphs: 1\n","Number of features: 1433\n","Number of classes: 7\n","\n","Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n","===========================================================================================================\n","Number of nodes: 2708\n","Number of edges: 10556\n","Average node degree: 3.90\n","Number of training nodes: 140\n","Training node label rate: 0.05\n","Has isolated nodes: False\n","Has self-loops: False\n","Is undirected: True\n"]}],"source":["from torch_geometric.datasets import Planetoid\n","from torch_geometric.transforms import NormalizeFeatures\n","\n","dataset = Planetoid(root='data/Planetoid', name='Cora', transform=NormalizeFeatures())\n","\n","print()\n","print(f'Dataset: {dataset}:')\n","print('======================')\n","print(f'Number of graphs: {len(dataset)}')\n","print(f'Number of features: {dataset.num_features}')\n","print(f'Number of classes: {dataset.num_classes}')\n","\n","data = dataset[0]  # Get the first graph object.\n","\n","print()\n","print(data)\n","print('===========================================================================================================')\n","\n","# Gather some statistics about the graph.\n","print(f'Number of nodes: {data.num_nodes}')\n","print(f'Number of edges: {data.num_edges}')\n","print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n","print(f'Number of training nodes: {data.train_mask.sum()}')\n","print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}')\n","print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n","print(f'Has self-loops: {data.has_self_loops()}')\n","print(f'Is undirected: {data.is_undirected()}')"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"eqWR0j_kIx67"},"source":["Overall, this dataset is quite similar to the previously used [`KarateClub`](https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html#torch_geometric.datasets.KarateClub) network.\n","We can see that the `Cora` network holds 2,708 nodes and 10,556 edges, resulting in an average node degree of 3.9.\n","For training this dataset, we are given the ground-truth categories of 140 nodes (20 for each class).\n","This results in a training node label rate of only 5%.\n","\n","In contrast to `KarateClub`, this graph holds the additional attributes `val_mask` and `test_mask`, which denotes which nodes should be used for validation and testing.\n","Furthermore, we make use of **[data transformations](https://pytorch-geometric.readthedocs.io/en/latest/notes/introduction.html#data-transforms) via `transform=NormalizeFeatures()`**.\n","Transforms can be used to modify your input data before inputting them into a neural network, *e.g.*, for normalization or data augmentation.\n","Here, we [row-normalize](https://pytorch-geometric.readthedocs.io/en/latest/modules/transforms.html#torch_geometric.transforms.NormalizeFeatures) the bag-of-words input feature vectors.\n","\n","We can further see that this network is undirected, and that there exists no isolated nodes (each document has at least one citation)."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"5IRdAELVKOl6"},"source":["## Training a Multi-layer Perception Network (MLP)\n","\n","In theory, we should be able to infer the category of a document solely based on its content, *i.e.* its bag-of-words feature representation, without taking any relational information into account.\n","\n","Let's verify that by constructing a simple MLP that solely operates on input node features (using shared weights across all nodes):"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Our Graph is Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n","\n","Number of features 1433\n","\n","Number of node 2708\n","Number of features.... again 1433\n","\n","Node feature vector example tensor([0., 0., 0.,  ..., 0., 0., 0.])\n","Features max 0.1111111119389534\n"]}],"source":["print(f\"Our Graph is {data}\")\n","print()\n","print(f\"Number of features {dataset.num_features}\")\n","print()\n","print(f\"Number of node {data.x.shape[0]}\")\n","print(f\"Number of features.... again {data.x.shape[1]}\")\n","print()\n","print(f\"Node feature vector example {data.x[0]}\")\n","print(f\"Features max {data.x[0].max()}\")"]},{"cell_type":"code","execution_count":52,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":214,"status":"ok","timestamp":1638795837951,"user":{"displayName":"Matthias Fey","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07888747774515796046"},"user_tz":-60},"id":"afXwPCA3KNoC","outputId":"d6dafb2a-1415-48b1-cfa2-7e0baf7f8491"},"outputs":[{"name":"stdout","output_type":"stream","text":["MLP(\n","  (lin1): Linear(in_features=1433, out_features=16, bias=True)\n","  (lin2): Linear(in_features=16, out_features=7, bias=True)\n",")\n"]}],"source":["import torch\n","from torch.nn import Linear\n","import torch.nn.functional as F\n","\n","\n","class MLP(torch.nn.Module):\n","    def __init__(self, hidden_channels):\n","        super().__init__()\n","        torch.manual_seed(12345)\n","        self.lin1 = Linear(dataset.num_features, hidden_channels)\n","        self.lin2 = Linear(hidden_channels, dataset.num_classes)\n","\n","    def forward(self, x):\n","        x = self.lin1(x)\n","        x = x.relu()\n","        x = F.dropout(x, p=0.5, training=self.training)\n","        x = self.lin2(x)\n","        return x\n","\n","model = MLP(hidden_channels=16)\n","print(model)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"L_PO9EEHL7J6"},"source":["Our MLP is defined by two linear layers and enhanced by [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html?highlight=relu#torch.nn.ReLU) non-linearity and [dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html?highlight=dropout#torch.nn.Dropout).\n","Here, we first reduce the 1433-dimensional feature vector to a low-dimensional embedding (`hidden_channels=16`), while the second linear layer acts as a classifier that should map each low-dimensional node embedding to one of the 7 classes.\n","\n","Let's train our simple MLP by following a similar procedure as described in [the first part of this tutorial](https://colab.research.google.com/drive/1h3-vJGRVloF5zStxL5I0rSy4ZUPNsjy8).\n","We again make use of the **cross entropy loss** and **Adam optimizer**.\n","This time, we also define a **`test` function** to evaluate how well our final model performs on the test node set (which labels have not been observed during training)."]},{"cell_type":"code","execution_count":53,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":300},"executionInfo":{"elapsed":1510,"status":"ok","timestamp":1638796134944,"user":{"displayName":"Matthias Fey","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07888747774515796046"},"user_tz":-60},"id":"0YgHcLXMLk4o","outputId":"71316c1f-addc-4aab-c61e-558c4ca139e5"},"outputs":[{"data":{"application/javascript":"google.colab.output.setIframeHeight(0, true, {maxHeight: 300})","text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"}],"source":["from IPython.display import Javascript  # Restrict height of output cell.\n","display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n","\n","model = MLP(hidden_channels=16)\n","criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)  # Define optimizer.\n","\n","def train():\n","      model.train()\n","      optimizer.zero_grad()  # Clear gradients.\n","      out = model(data.x)  # Perform a single forward pass. NO EDGES INFO IS PROVIDED\n","      loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n","      loss.backward()  # Derive gradients.\n","      optimizer.step()  # Update parameters based on gradients.\n","      return loss\n","\n","def test():\n","      model.eval()\n","      out = model(data.x)\n","      pred = out.argmax(dim=1)  # Use the class with highest probability.\n","      test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n","      test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n","      return test_acc\n"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Accuracy: 0.1030\n"]}],"source":["test_acc = test()\n","print(f'Test Accuracy: {test_acc:.4f}')"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 001, Loss: 1.9609\n","Epoch: 002, Loss: 1.9558\n","Epoch: 003, Loss: 1.9485\n","Epoch: 004, Loss: 1.9413\n","Epoch: 005, Loss: 1.9297\n","Epoch: 006, Loss: 1.9266\n","Epoch: 007, Loss: 1.9124\n","Epoch: 008, Loss: 1.9024\n","Epoch: 009, Loss: 1.8985\n","Epoch: 010, Loss: 1.8849\n","Epoch: 011, Loss: 1.8753\n","Epoch: 012, Loss: 1.8684\n","Epoch: 013, Loss: 1.8483\n","Epoch: 014, Loss: 1.8465\n","Epoch: 015, Loss: 1.8115\n","Epoch: 016, Loss: 1.8143\n","Epoch: 017, Loss: 1.7900\n","Epoch: 018, Loss: 1.7985\n","Epoch: 019, Loss: 1.7639\n","Epoch: 020, Loss: 1.7383\n","Epoch: 021, Loss: 1.7229\n","Epoch: 022, Loss: 1.7452\n","Epoch: 023, Loss: 1.7000\n","Epoch: 024, Loss: 1.6855\n","Epoch: 025, Loss: 1.6691\n","Epoch: 026, Loss: 1.6479\n","Epoch: 027, Loss: 1.6284\n","Epoch: 028, Loss: 1.5960\n","Epoch: 029, Loss: 1.5562\n","Epoch: 030, Loss: 1.5396\n","Epoch: 031, Loss: 1.5488\n","Epoch: 032, Loss: 1.5224\n","Epoch: 033, Loss: 1.4665\n","Epoch: 034, Loss: 1.4885\n","Epoch: 035, Loss: 1.4399\n","Epoch: 036, Loss: 1.4035\n","Epoch: 037, Loss: 1.4265\n","Epoch: 038, Loss: 1.3742\n","Epoch: 039, Loss: 1.3742\n","Epoch: 040, Loss: 1.3391\n","Epoch: 041, Loss: 1.3089\n","Epoch: 042, Loss: 1.2701\n","Epoch: 043, Loss: 1.2487\n","Epoch: 044, Loss: 1.2711\n","Epoch: 045, Loss: 1.2219\n","Epoch: 046, Loss: 1.1674\n","Epoch: 047, Loss: 1.1749\n","Epoch: 048, Loss: 1.1392\n","Epoch: 049, Loss: 1.1493\n","Epoch: 050, Loss: 1.1060\n","Epoch: 051, Loss: 1.0598\n","Epoch: 052, Loss: 1.0722\n","Epoch: 053, Loss: 1.0351\n","Epoch: 054, Loss: 1.0588\n","Epoch: 055, Loss: 0.9974\n","Epoch: 056, Loss: 0.9684\n","Epoch: 057, Loss: 0.8953\n","Epoch: 058, Loss: 0.9257\n","Epoch: 059, Loss: 0.9519\n","Epoch: 060, Loss: 0.9173\n","Epoch: 061, Loss: 0.8916\n","Epoch: 062, Loss: 0.8910\n","Epoch: 063, Loss: 0.8902\n","Epoch: 064, Loss: 0.9588\n","Epoch: 065, Loss: 0.8639\n","Epoch: 066, Loss: 0.8241\n","Epoch: 067, Loss: 0.8078\n","Epoch: 068, Loss: 0.7704\n","Epoch: 069, Loss: 0.7910\n","Epoch: 070, Loss: 0.7889\n","Epoch: 071, Loss: 0.7019\n","Epoch: 072, Loss: 0.7265\n","Epoch: 073, Loss: 0.8248\n","Epoch: 074, Loss: 0.7720\n","Epoch: 075, Loss: 0.7473\n","Epoch: 076, Loss: 0.7014\n","Epoch: 077, Loss: 0.7245\n","Epoch: 078, Loss: 0.7311\n","Epoch: 079, Loss: 0.6861\n","Epoch: 080, Loss: 0.6899\n","Epoch: 081, Loss: 0.6589\n","Epoch: 082, Loss: 0.7592\n","Epoch: 083, Loss: 0.6846\n","Epoch: 084, Loss: 0.6654\n","Epoch: 085, Loss: 0.6847\n","Epoch: 086, Loss: 0.6332\n","Epoch: 087, Loss: 0.6185\n","Epoch: 088, Loss: 0.5642\n","Epoch: 089, Loss: 0.6276\n","Epoch: 090, Loss: 0.6685\n","Epoch: 091, Loss: 0.5877\n","Epoch: 092, Loss: 0.6406\n","Epoch: 093, Loss: 0.6497\n","Epoch: 094, Loss: 0.6774\n","Epoch: 095, Loss: 0.6194\n","Epoch: 096, Loss: 0.5723\n","Epoch: 097, Loss: 0.5686\n","Epoch: 098, Loss: 0.6565\n","Epoch: 099, Loss: 0.6382\n"]}],"source":["for epoch in range(1, 100):\n","    loss = train()\n","    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"kG4IKy9YOLGF"},"source":["After training the model, we can call the `test` function to see how well our model performs on unseen labels.\n","Here, we are interested in the accuracy of the model, *i.e.*, the ratio of correctly classified nodes:"]},{"cell_type":"code","execution_count":57,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":229,"status":"ok","timestamp":1638796139382,"user":{"displayName":"Matthias Fey","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07888747774515796046"},"user_tz":-60},"id":"dBBCeLlAL0oL","outputId":"82b69943-feb5-4df4-fef0-5178682ba47e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Accuracy: 0.5710\n"]}],"source":["test_acc = test()\n","print(f'Test Accuracy: {test_acc:.4f}')"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of classes: 7\n","Number of classes: tensor([0, 1, 2, 3, 4, 5, 6])\n"]}],"source":["print(f'Number of classes: {dataset.num_classes}')\n","print(f'Number of classes: {data.y.unique()}')"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"_jjJOB-VO-cw"},"source":["As one can see, our MLP performs rather bad with only about 59% test accuracy.\n","But why does the MLP do not perform better?\n","The main reason for that is that this model suffers from heavy overfitting due to only having access to a **small amount of training nodes**, and therefore generalizes poorly to unseen node representations.\n","\n","It also fails to incorporate an important bias into the model: **Cited papers are very likely related to the category of a document**.\n","That is exactly where Graph Neural Networks come into play and can help to boost the performance of our model.\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"_OWGw54wRd98"},"source":["## Training a Graph Neural Network (GNN)\n","\n","We can easily convert our MLP to a GNN by swapping the `torch.nn.Linear` layers with PyG's GNN operators.\n","\n","Following-up on [the first part of this tutorial](https://colab.research.google.com/drive/1h3-vJGRVloF5zStxL5I0rSy4ZUPNsjy8), we replace the linear layers by the [`GCNConv`](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GCNConv) module.\n","To recap, the **GCN layer** ([Kipf et al. (2017)](https://arxiv.org/abs/1609.02907)) is defined as\n","\n","$$\n","\\mathbf{h}_v^{(\\ell + 1)} = \\mathbf{W}^{(\\ell + 1)} \\sum_{w \\in \\mathcal{N}(v) \\, \\cup \\, \\{ v \\}} \\frac{1}{c_{w,v}} \\cdot \\mathbf{h}_w^{(\\ell)}\n","$$\n","\n","where $\\mathbf{W}^{(\\ell + 1)}$ denotes a trainable weight matrix of shape `[num_output_features, num_input_features]` and $c_{w,v}$ refers to a fixed normalization coefficient for each edge.\n","In contrast, a single `Linear` layer is defined as\n","\n","$$\n","\\mathbf{h}_v^{(\\ell + 1)} = \\mathbf{W}^{(\\ell + 1)} \\mathbf{h}_v^{(\\ell)}\n","$$\n","\n","which does not make use of neighboring node information."]},{"cell_type":"code","execution_count":58,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":235,"status":"ok","timestamp":1638796009700,"user":{"displayName":"Matthias Fey","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07888747774515796046"},"user_tz":-60},"id":"fmXWs1dKIzD8","outputId":"d3936b27-6d6b-4159-fb0d-08e0e381a715"},"outputs":[{"name":"stdout","output_type":"stream","text":["GCN(\n","  (conv1): GCNConv(1433, 16)\n","  (conv2): GCNConv(16, 7)\n",")\n"]}],"source":["from torch_geometric.nn import GCNConv\n","\n","\n","class GCN(torch.nn.Module):\n","    def __init__(self, hidden_channels):\n","        super().__init__()\n","        torch.manual_seed(1234567)\n","        self.conv1 = GCNConv(dataset.num_features, hidden_channels)\n","        self.conv2 = GCNConv(hidden_channels, dataset.num_classes)\n","\n","    def forward(self, x, edge_index):\n","        x = self.conv1(x, edge_index)\n","        x = x.relu()\n","        x = F.dropout(x, p=0.5, training=self.training)\n","        x = self.conv2(x, edge_index)\n","        return x\n","\n","model = GCN(hidden_channels=16)\n","print(model)"]},{"cell_type":"code","execution_count":59,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":300},"executionInfo":{"elapsed":1893,"status":"ok","timestamp":1638796241305,"user":{"displayName":"Matthias Fey","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07888747774515796046"},"user_tz":-60},"id":"p3TAi69zI1bO","outputId":"4ad25e08-92dd-4396-f144-7a98126d6bf4"},"outputs":[{"data":{"application/javascript":"google.colab.output.setIframeHeight(0, true, {maxHeight: 300})","text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 001, Loss: 1.9465\n","Epoch: 002, Loss: 1.9419\n","Epoch: 003, Loss: 1.9363\n","Epoch: 004, Loss: 1.9290\n","Epoch: 005, Loss: 1.9199\n","Epoch: 006, Loss: 1.9140\n","Epoch: 007, Loss: 1.9079\n","Epoch: 008, Loss: 1.8992\n","Epoch: 009, Loss: 1.8876\n","Epoch: 010, Loss: 1.8764\n","Epoch: 011, Loss: 1.8656\n","Epoch: 012, Loss: 1.8626\n","Epoch: 013, Loss: 1.8460\n","Epoch: 014, Loss: 1.8329\n","Epoch: 015, Loss: 1.8225\n","Epoch: 016, Loss: 1.8167\n","Epoch: 017, Loss: 1.7995\n","Epoch: 018, Loss: 1.7878\n","Epoch: 019, Loss: 1.7716\n","Epoch: 020, Loss: 1.7568\n","Epoch: 021, Loss: 1.7563\n","Epoch: 022, Loss: 1.7342\n","Epoch: 023, Loss: 1.7092\n","Epoch: 024, Loss: 1.7015\n","Epoch: 025, Loss: 1.6671\n","Epoch: 026, Loss: 1.6757\n","Epoch: 027, Loss: 1.6609\n","Epoch: 028, Loss: 1.6355\n","Epoch: 029, Loss: 1.6339\n","Epoch: 030, Loss: 1.6102\n","Epoch: 031, Loss: 1.5964\n","Epoch: 032, Loss: 1.5721\n","Epoch: 033, Loss: 1.5570\n","Epoch: 034, Loss: 1.5445\n","Epoch: 035, Loss: 1.5093\n","Epoch: 036, Loss: 1.4889\n","Epoch: 037, Loss: 1.4776\n","Epoch: 038, Loss: 1.4704\n","Epoch: 039, Loss: 1.4263\n","Epoch: 040, Loss: 1.3972\n","Epoch: 041, Loss: 1.3873\n","Epoch: 042, Loss: 1.3479\n","Epoch: 043, Loss: 1.3485\n","Epoch: 044, Loss: 1.3739\n","Epoch: 045, Loss: 1.3343\n","Epoch: 046, Loss: 1.3277\n","Epoch: 047, Loss: 1.2770\n","Epoch: 048, Loss: 1.2651\n","Epoch: 049, Loss: 1.2347\n","Epoch: 050, Loss: 1.2543\n","Epoch: 051, Loss: 1.1622\n","Epoch: 052, Loss: 1.1483\n","Epoch: 053, Loss: 1.1535\n","Epoch: 054, Loss: 1.1912\n","Epoch: 055, Loss: 1.0880\n","Epoch: 056, Loss: 1.1374\n","Epoch: 057, Loss: 1.0657\n","Epoch: 058, Loss: 1.0748\n","Epoch: 059, Loss: 1.0654\n","Epoch: 060, Loss: 1.0201\n","Epoch: 061, Loss: 0.9967\n","Epoch: 062, Loss: 1.0499\n","Epoch: 063, Loss: 1.0116\n","Epoch: 064, Loss: 0.9945\n","Epoch: 065, Loss: 0.9499\n","Epoch: 066, Loss: 0.9465\n","Epoch: 067, Loss: 0.9633\n","Epoch: 068, Loss: 0.9137\n","Epoch: 069, Loss: 0.9168\n","Epoch: 070, Loss: 0.8818\n","Epoch: 071, Loss: 0.8984\n","Epoch: 072, Loss: 0.8301\n","Epoch: 073, Loss: 0.8664\n","Epoch: 074, Loss: 0.8560\n","Epoch: 075, Loss: 0.8457\n","Epoch: 076, Loss: 0.8306\n","Epoch: 077, Loss: 0.8333\n","Epoch: 078, Loss: 0.8155\n","Epoch: 079, Loss: 0.7878\n","Epoch: 080, Loss: 0.8277\n","Epoch: 081, Loss: 0.7880\n","Epoch: 082, Loss: 0.7829\n","Epoch: 083, Loss: 0.7829\n","Epoch: 084, Loss: 0.7633\n","Epoch: 085, Loss: 0.7862\n","Epoch: 086, Loss: 0.7497\n","Epoch: 087, Loss: 0.7760\n","Epoch: 088, Loss: 0.7419\n","Epoch: 089, Loss: 0.6595\n","Epoch: 090, Loss: 0.6746\n","Epoch: 091, Loss: 0.7432\n","Epoch: 092, Loss: 0.6609\n","Epoch: 093, Loss: 0.6607\n","Epoch: 094, Loss: 0.6884\n","Epoch: 095, Loss: 0.6596\n","Epoch: 096, Loss: 0.6456\n","Epoch: 097, Loss: 0.6383\n","Epoch: 098, Loss: 0.7031\n","Epoch: 099, Loss: 0.6437\n","Epoch: 100, Loss: 0.6375\n"]}],"source":["from IPython.display import Javascript  # Restrict height of output cell.\n","display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n","\n","model = GCN(hidden_channels=16)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","def train():\n","      model.train()\n","      optimizer.zero_grad()  # Clear gradients.\n","      out = model(data.x, data.edge_index)  # Perform a single forward pass.\n","      loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n","      loss.backward()  # Derive gradients.\n","      optimizer.step()  # Update parameters based on gradients.\n","      return loss\n","\n","def test():\n","      model.eval()\n","      out = model(data.x, data.edge_index)\n","      pred = out.argmax(dim=1)  # Use the class with highest probability.\n","      test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n","      test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n","      return test_acc\n","\n","\n","for epoch in range(1, 101):\n","    loss = train()\n","    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"opBBGQHqg5ZO"},"source":["After training the model, we can check its test accuracy:"]},{"cell_type":"code","execution_count":61,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":216,"status":"ok","timestamp":1638796252659,"user":{"displayName":"Matthias Fey","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07888747774515796046"},"user_tz":-60},"id":"8zOh6IIeI3Op","outputId":"1495b4b5-77c3-4374-ced1-5ad9d352a465"},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Accuracy: 0.8110\n"]}],"source":["test_acc = test()\n","print(f'Test Accuracy: {test_acc:.4f}')"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"yhofzjaqhfY2"},"source":["**There it is!**\n","By simply swapping the linear layers with GNN layers, we can reach **81.5% of test accuracy**!\n","This is in stark contrast to the 59% of test accuracy obtained by our MLP, indicating that relational information plays a crucial role in obtaining better performance.\n","\n","We can also verify that once again by looking at the output embeddings of our **trained** model, which now produces a far better clustering of nodes of the same category."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pcr9joFQ6Mri"},"outputs":[],"source":["from torch_geometric.nn import GATConv\n","\n","\n","class GAT(torch.nn.Module):\n","    def __init__(self, hidden_channels, heads):\n","        super().__init__()\n","        torch.manual_seed(1234567)\n","        self.conv1 = GATConv(dataset.num_features, hidden_channels, heads=heads)  # TODO\n","        self.conv2 = GATConv(hidden_channels*heads, dataset.num_classes)  # TODO\n","\n","    def forward(self, x, edge_index):\n","        x = F.dropout(x, p=0.6, training=self.training)\n","        x = self.conv1(x, edge_index)\n","        x = F.elu(x)\n","        x = F.dropout(x, p=0.6, training=self.training)\n","        x = self.conv2(x, edge_index)\n","        return x\n","\n","model = GAT(hidden_channels=8, heads=8)\n","print(model)\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","def train():\n","      model.train()\n","      optimizer.zero_grad()  # Clear gradients.\n","      out = model(data.x, data.edge_index)  # Perform a single forward pass.\n","      print(out[data.train_mask][0].argmax())\n","      print(data.y[data.train_mask][0])\n","      loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n","      loss.backward()  # Derive gradients.\n","      optimizer.step()  # Update parameters based on gradients.\n","      return loss\n","\n","def test(mask):\n","      model.eval()\n","      out = model(data.x, data.edge_index)\n","      pred = out.argmax(dim=1)  # Use the class with highest probability.\n","      correct = pred[mask] == data.y[mask]  # Check against ground-truth labels.\n","      acc = int(correct.sum()) / int(mask.sum())  # Derive ratio of correct predictions.\n","      return acc\n","\n","\n","for epoch in range(1, 50):\n","    loss = train()\n","    val_acc = test(data.val_mask)\n","    test_acc = test(data.test_mask)\n","    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}')"]}],"metadata":{"colab":{"provenance":[{"file_id":"14OvFnAXggxB8vM4e8vSURUp1TaKnovzX","timestamp":1683109389259}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":0}
